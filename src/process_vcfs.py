import argparse
import multiprocessing as mp
import os
import subprocess
from glob import glob
from itertools import cycle

import yaml


def read_multivcf(input_vcf):
    """Reads in file and returns as list of strings."""
    with open(input_vcf, "r") as input_file:
        raw_lines = [i.strip() for i in input_file.readlines()]

    return raw_lines


def split_multivcf(vcf_lines, header):
    """Splits the lines of multi-vcf file into list of vcf entries by <header> using itertools."""
    header_idxs = [i for i in range(len(vcf_lines)) if vcf_lines[i] == header]

    split_vcfs = []
    for idx in range(len(header_idxs[:-1])):
        print(idx)
        split_vcfs.append(vcf_lines[header_idxs[idx] : header_idxs[idx + 1]])

    split_vcfs.append(vcf_lines[header_idxs[-1] :])

    return split_vcfs


def make_vcf_dir(input_vcf):
    """Creates directory named after vcf basename."""
    dirname = os.path.basename(input_vcf).split(".")[0]
    dirpath = os.path.dirname(input_vcf)
    vcf_dir = os.path.join(dirpath, dirname)
    os.makedirs(vcf_dir, exist_ok=True)

    return vcf_dir


def write_vcfs(vcf_lines, vcf_dir):
    """Writes list of vcf entries to numerically-sorted vcf files."""
    for idx, lines in enumerate(vcf_lines):
        with open(os.path.join(vcf_dir, f"{idx}.vcf"), "w") as outfile:
            outfile.writelines("\n".join(lines))


def index_vcf(vcf):
    # Is it safe? Probably not. Does it work better than futzing with gzip pipes? Definitely.
    subprocess.run(f"bgzip -c {vcf} > {vcf}.gz".split(), shell=True)
    subprocess.run(f"bcftools index {vcf}.gz", shell=True)


def merge_vcfs(vcf_dir):
    subprocess.run(
        f"bcftools merge -Oz --force-samples -0 ${vcf_dir}/*.vcf.gz > ${vcf_dir}/merged.vcf.gz"
    )


def get_ua():
    ap = argparse.ArgumentParser(
        description="Splits multi-vcf files from SLiM into a directory containing numerically-sorted time series vcf files."
    )
    ap.add_argument(
        "-i",
        "--input-vcf",
        required=True,
        type=str,
        dest="input_vcf",
        help="File containing multiple VCF entries from SLiM's `outputVCFSample` with `append=T`.",
    )
    ap.add_argument(
        "--vcf-header",
        required=False,
        type=str,
        default="##fileformat=VCFv4.2",
        dest="vcf_header",
        help="String that tops VCF header, used to split entries to new files.",
    )

    ap.add_argument(
        "--delete",
        required=False,
        action="store_true",
        dest="delete_vcf",
        help="Whether or not to delete the original multi-vcf file after splitting.",
    )
    ap.add_argument(
        "--threads",
        required=False,
        type=int,
        default=mp.cpu_count(),
        dest="threads",
        help="Number of processes to parallelize across.",
    )
    return ap.parse_args()


def parse_ua():
    uap = argparse.ArgumentParser(
        description="Creates training data from simulated merged vcfs after process_vcfs.py has been run."
    )
    uap.add_argument(
        "--vcf-header",
        required=False,
        type=str,
        default="##fileformat=VCFv4.2",
        dest="vcf_header",
        help="String that tops VCF header, used to split entries to new files.",
    )
    subparsers = uap.add_subparsers(dest="config_format")
    subparsers.required = True
    yaml_parser = subparsers.add_parser("yaml")
    yaml_parser.add_argument(
        "-y",
        "--yaml",
        metavar="YAML CONFIG",
        dest="yaml_file",
        help="YAML config file with all cli options defined.",
    )

    cli_parser = subparsers.add_parser("cli")
    cli_parser.add_argument(
        "-w",
        "--work-dir",
        dest="work_dir",
        type=str,
        help="Directory used as work dir for simulate modules. Should contain simulated vcfs processed using process_vcf.py.",
        required=False,
        default=os.getcwd(),
    )
    cli_parser.add_argument(
        "--threads",
        required=False,
        type=int,
        default=mp.cpu_count(),
        dest="threads",
        help="Number of processes to parallelize across.",
    )
    return uap.parse_args()


def read_config(yaml_file):
    """Reads in the YAML config file."""
    with open(yaml_file, "r") as infile:
        yamldata = yaml.safe_load(infile)

    return yamldata


def worker(input_vcf, vcf_header):
    # Split into multiples after SLiM just concats to same file
    raw_lines = read_multivcf(input_vcf)
    split_lines = split_multivcf(raw_lines, vcf_header)

    # Creates subdir for each rep
    vcf_dir = make_vcf_dir(input_vcf)
    write_vcfs(split_lines, vcf_dir)

    # Now index and merge
    [index_vcf(vcf) for vcf in glob(f"{vcf_dir}/*.vcf")]
    merge_vcfs(vcf_dir)


def main():
    """Splits multivcf generated by SLiM using simulate.py and creates a merged/indexed vcf to create training data with."""
    ua = get_ua()
    if ua.config_format == "yaml":
        yaml_data = read_config(ua.yaml_file)
        work_dir, threads = (
            yaml_data["work dir"],
            yaml_data["threads"],
        )

    elif ua.config_format == "cli":
        work_dir, threads, vcf_header = ua.work_dir, ua.threads, ua.vcf_header

    input_vcfs = glob(f"{work_dir}/vcfs/*/*.multivcf")
    pool = mp.Pool(threads)
    pool.starmap(worker, zip(input_vcfs, cycle(vcf_header)))


if __name__ == "__main__":
    main()
