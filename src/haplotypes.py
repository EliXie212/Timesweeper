import argparse
import multiprocessing as mp
import os, sys
from glob import glob
from itertools import cycle
from logging import warning
import numpy as np
from tqdm import tqdm

# This iteration of the haplotype module has had most of the arbitrary options removed
# Meaning that the shape and sampling frequency of simulations is dependent on the schema generated by inject_slim based on whatever data was used to generate the simulation
# PhysLen still needs to be supplied, and is found in the SLiM script used to generate data


class MsHandler:
    """Handles haplotype-tracked MS-style formatting from a standard SLiM output file.
    Runner function is parse_slim for easy tracking."""

    def __init__(self, mutfile, tol, physLen):
        self.mutfile = mutfile
        self.tol = tol
        self.physLen = physLen

    def parse_slim(self):
        """
        Runs all necessary steps to parse SLiM output and format it for hfs creation.

        Returns:
            list[str]: List of "lines" of a typical ms-format output. Used to be output as an intermediate file, but is now just passed as a list of str for parsing.
        """
        with open(self.mutfile, "r") as infile:
            lines = [i.strip() for i in infile.readlines()]

        cleaned_lines = self.remove_restarts(lines)
        mutations, genomes, samp_sizes, gens_sampled = self.readSampleOutFromSlimRun(
            cleaned_lines
        )
        newMutLocs = self.get_mutLocs(mutations)
        unfilteredMuts = self.buildMutationPosMapping(newMutLocs)
        polyMuts = self.removeMonomorphic(unfilteredMuts, genomes)
        positionsStr = self.buildPositionsStr(polyMuts)

        # Iterate through timepoints, mutations is just a length indicator at this point
        segsitesStr = f"segsites: {len(polyMuts)}"
        haps = self.make_haps(polyMuts, genomes)
        out_ms = self.emitMsEntry(positionsStr, segsitesStr, haps)

        return out_ms, samp_sizes, gens_sampled

    def remove_restarts(self, lines):
        gens = []
        out_idxs = []
        for idx, line in enumerate(lines):
            if "#OUT" in line:
                gens.append(int(line.split(" ")[1]))
                out_idxs.append(idx)

        min_gen_inds = [i for i, x in enumerate(gens) if x == min(gens)]
        if (
            len(min_gen_inds) > 1
        ):  # Get indices of any duplicated gens - spans gens and out_idxs
            # Remove lines between the first and last occurence
            # Only want to keep the ones after the restart
            # Technically only restarts should happen at the dumpfile ggen, but this is flexible for anything I suppose
            del lines[0 : out_idxs[max(min_gen_inds)]]

        return lines

    def readSampleOutFromSlimRun(self, lines):
        """
        Adds genomes and mutations to object-wide dicts every time a new one is encountered.
        Scans through each line of SLiM output, at the end of a sample it will collate genomes and mutation info and store in dicts.
        """
        mode = 0
        mutations = {}
        genomes = []
        samp_sizes_list = []
        sampleText = []
        gens_sampled = []
        for idx, line in enumerate(lines):
            if mode == 0:
                if "#OUT" in line:
                    samp_sizes_list.append(int(line.split(" ")[4]))
                    gens_sampled.append(int(line.split(" ")[1]))
                    if idx != 0:
                        all_samp_genomes = self.addMutationsAndGenomesFromSample(
                            sampleText, mutations,
                        )
                        genomes.extend(all_samp_genomes)

                    sampleText = []

                else:
                    sampleText.append(line)

        # Last one
        all_samp_genomes = self.addMutationsAndGenomesFromSample(sampleText, mutations)
        genomes.extend(all_samp_genomes)

        return mutations, genomes, samp_sizes_list, gens_sampled

    def addMutationsAndGenomesFromSample(self, sampleText, mutations):
        """
        Maps mutation IDs to chromosomes that contain them, resulting in a genotype string that is added to the genomes list.

        Args:
            sampleText (list[str]): Lines of SLiM output relating to one timepoint sample from a series.
            mutations (dict[int]): Dict of mutation locations binned by their ID in the chromosome being sampled.
            genomes (list[set(int)]): List of genome IDs for each sample that have muts

        Mutations and Genomes are added in-scope.
        """
        mode = 0
        idMapping = {}
        genomes = []
        for line in sampleText:
            if mode == 0:
                if "Mutations" in line:
                    mode = 1
            elif mode == 1:
                if "Genomes" in line:
                    mode = 2
                elif len(line.strip().split()) == 9:
                    (
                        tempId,
                        permId,
                        mutType,
                        pos,
                        selCoeff,
                        domCoeff,
                        subpop,
                        gen,
                        numCopies,
                    ) = line.strip().split()
                    pos = int(pos)
                    if not pos in mutations:
                        mutations[pos] = {}
                    mutations[pos][permId] = 1
                    idMapping[tempId] = permId
            elif mode == 2:
                line = line.strip().split()
                gId, auto = line[:2]
                mutLs = line[2:]
                genomes.append(set([idMapping[x] for x in mutLs]))

        return genomes

    def get_mutLocs(self, mutations):
        """
        Build new mutation map based on windows of mutations.

        Args:
            mutations (dict[int]): Dict of mutations binned by location.

        Returns:
            list[tuple(int, int)]: List of paired mutation (positions, IDs) in new format.
        """
        newMutLocs = []
        for mutPos in mutations:
            if len(mutations[mutPos]) == 1:
                mutId = list(mutations[mutPos].keys())[0]
                newMutLocs.append((mutPos, mutId))
            else:
                firstPos = mutPos - self.tol
                lastPos = mutPos + self.tol
                interval = (lastPos - firstPos) / (len(mutations[mutPos]) - 1)
                currPos = firstPos
                for mutId in mutations[mutPos]:
                    newMutLocs.append((currPos, mutId))
                    currPos += interval

        return newMutLocs

    def buildMutationPosMapping(self, mutLocs):
        """
        Creates new mapping relative to length of chromosome, adds to information tuple for mutation.

        Args:
            mutLocs list[tuple(int, int)]: List of paired mutation (positions, IDs) in new format.
            physLen int: Length of chromosomes
        Returns:
            list[tuple(int, int, float, int)]: Tuples of (newID, abs position, continuous position, permID).
        """
        mutMapping = []
        mutLocs.sort()
        for i in range(len(mutLocs)):
            pos, mutId = mutLocs[i]
            contPos = pos / self.physLen
            mutMapping.append((i, pos, contPos, mutId))

        return mutMapping

    def removeMonomorphic(self, allMuts, genomes):
        """
        INSERT FIXED ALLELES BEFORE HERE
        Removes singletons by selecting only mutations that are polymorphic.

        Args:
            allMuts (list[tuple(int, int, float, int)]): Tuples of (newID, abs position, continuous position, permID)

        Returns:
            list[tuple(int, int, float, int)]: Tuples of (newID, abs position, continuous position, permID) for polymorphic mutations only.
        """
        newMuts = []
        newLocI = 0
        for locI, loc, contLoc, mutId in allMuts:
            freq = self.getFreq(mutId, genomes)
            if freq > 0 and freq < len(genomes):
                newMuts.append((newLocI, loc, contLoc, mutId))
                newLocI += 1

        return newMuts

    def getFreq(self, mut, genomes):
        """
        Calculate ocurrence of a mutation in each genome.

        Args:
            mut (tuple): Mutation information to query against the genome list.

        Returns:
            int: Number of times input mutation appears in all genomes.
        """
        mutCount = 0
        for genome in genomes:
            if mut in genome:
                mutCount += 1

        return mutCount

    def buildPositionsStr(self, muts):
        """
        Uses new mutation locations to build an MS-style mutation positions string.

        Args:
            muts (list[tuple]): Tuples of (newID, abs position, continuous position, permID) for each mutation.

        Returns:
            str: ms-style chromosome mutation positions string for use in downstream parsing.
        """
        positionsStr = []
        for _, locationDiscrete, _, mutId in muts:
            positionsStr.append(f"{locationDiscrete}.{mutId}")

        return "positions: " + " ".join(positionsStr)

    def make_haps(self, polyMuts, sampled_genomes):
        """
        Creates genotype 0/1 strings for each haplotype in ms-style format.

        Args:
            polyMuts (list[tuple]): Polymorphic mutations with ID, location, and permID fields.

        Returns:
            list[str]: All haplotype genotype strings for a given sample.
        """
        haps = []

        for i in range(len(sampled_genomes)):
            haps.append(["0"] * len(polyMuts))

        for i in range(len(sampled_genomes)):
            for locI, loc, contLoc, mutId in polyMuts:
                if mutId in sampled_genomes[i]:
                    haps[i][locI] = "1"

        return haps

    def emitMsEntry(self, positionsStr, segsitesStr, haps):
        """
        Writes a list of strings that is equivalent to the lines in an ms-formatted output.
        Can be edited to output to file instead easily.

        Args:
            positionsStr (str): Str of all positions with segsites
            segsitesStr (str): Str of number of segsites total in MS entry
            haps (list[str]]): All haplotype genotype strings for a given sample.

        Returns:
            List[str]: Expected MS output format for entire time series of sampled points and haps.
        """

        ms = []
        ms.append(f"slim {len(haps)} 1")
        ms.append("foo")
        ms.append("//")
        ms.append(segsitesStr)
        ms.append(positionsStr)
        for line in haps:
            ms.append("".join(line))

        return ms


class HapHandler:
    """
    Handles haplotype frequency spectrum generation, sorting, and output.
    Structure is very nested, user-facing function is readAndSplitMsData.
    """

    def __init__(self, hap_ms, maxSnps, samp_sizes, gens_sampled):
        self.hap_ms = hap_ms
        self.maxSnps = maxSnps
        self.samp_sizes = samp_sizes
        self.gens_sampled = gens_sampled

    def readAndSplitMsData(self, inFileName):
        """Runner function that allows for broad exception catching from nested functions."""
        try:
            sampleHaps = self.readMsData()
            hapMats = self.getTimeSeriesHapFreqs(sampleHaps)
            X = np.array(hapMats, dtype="float32")

            return X, "/".join([inFileName.split("/")[-3], inFileName.split("/")[-1]])

        except Exception as e:
            print(
                "couldn't make {} because of: {}".format(
                    inFileName.split("/")[-1].split(".")[0]
                ),
                e,
            )
            return None, None

    def readMsData(self):
        """
        Iterates through haplotype-tracked MS entry and creates haplotype matrices.


        Returns:
            list[list[float]]: Haplotype frequency spectrums for all timepoints; sorted by most common freq at any sampling point in series.
        """
        readMode = 0
        for line in self.hap_ms:
            if readMode == 0:
                if line.startswith("positions:"):
                    readMode = 1
                    currHaps = []
                elif line.startswith("segsites:"):
                    numSnps = int(line.strip().split()[-1])
                    if numSnps >= self.maxSnps:
                        start = int((numSnps - self.maxSnps) / 2)
                        end = start + self.maxSnps
                    else:
                        start, end = 0, numSnps
            elif readMode == 1:
                line = line.strip()
                if not line:
                    pass
                else:
                    if line[0] in ["0", "1"]:
                        currHaps.append(line[start:end])

        return currHaps

    def getTimeSeriesHapFreqs(self, currHaps):
        """
        Build haplotype frequency spectrum for a time series.

        Args:
            currHaps (list[str]): List of haplotypes read from MS entry.

        Returns:
            list[float]: Haplotype frequency spectrum for a single timepoint sorted by the most common hap in entire set.
        """
        tsHapDicts = self.calcHapFreqs(currHaps)

        sortedTSHapsDict = self.sortHaps(tsHapDicts, currHaps)
        hapsArr = np.stack(list(sortedTSHapsDict.values()))

        hfs = np.zeros((len(currHaps), len(self.samp_sizes)))
        hfs[: len(sortedTSHapsDict), :] = hapsArr

        return hfs

    def calcHapFreqs(self, haps):
        """
        Iterates through haplotype counts in entire series to find the most common hap.

        Args:
            haps (list[str]): List of haplotypes, each is a 1D genotype string

        Returns:
            List[Dict]: [timepoints * {hap: freq}]
        """
        # Calculate haplotype frequency for each haplotype, iterate through each timepoint
        allFreqs = []
        i = 0
        for j in self.samp_sizes:
            freqsInSamp = {}
            for hap in haps[i : i + j]:
                if not hap in freqsInSamp:
                    freqsInSamp[hap] = 0
                freqsInSamp[hap] += 1 / j
            allFreqs.append(freqsInSamp)
            i += j

        return allFreqs

    def sortHaps(self, tsHapDicts, haps):
        # Calculate haplotype freq change from the max to min freq of each hap's time series
        # allFreqs is list of dicts: [timesteps * {haps: freqs}]
        hapsDict = {}  # Flat haps structure just for freqs
        freqChanges = {}
        for hap in haps:
            hapsDict[hap] = [
                tsHapDicts[i][hap]
                for i in range(len(tsHapDicts))
                if hap in tsHapDicts[i]
            ]
            if len(hapsDict[hap]) > 1:
                freqChanges[hap] = max(hapsDict[hap]) - min(hapsDict[hap])
            else:
                freqChanges[hap] = hapsDict[hap][0]

        sortedFreqChanges = {
            k: v for k, v in sorted(freqChanges.items(), key=lambda item: item[1])
        }

        sortedTSHapsDict = {hap: [] for hap in sortedFreqChanges.keys()}
        for timeStep in range(len(tsHapDicts)):
            for hap in sortedTSHapsDict.keys():
                # Now we can populate HFS in order
                if hap in tsHapDicts[timeStep]:
                    sortedTSHapsDict[hap].append(tsHapDicts[timeStep][hap])
                else:  # Fill in 0 where not present for easy HFS build
                    sortedTSHapsDict[hap].append(0.0)

        return sortedTSHapsDict


def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Haplotype frequency spectrum feature vector preparation.\
            Each run will result in an npz file named {samp_frequency}_{samp_size}.npz"
    )

    parser.add_argument(
        "-i",
        "--input-dir",
        metavar="INPUT_DIRECTORY",
        help="Base mutation type (hard/soft/etc) directory with *.pop files to create feature vectors from.",
        dest="in_dir",
        type=str,
        required=False,
    )

    parser.add_argument(
        "-p",
        "--phys-len",
        metavar="PHYS_LEN",
        help="Length of chromosome being simulated for training, will be specified in the stdpopsim call as well as in all SLiM scripts.",
        dest="physLen",
        default=100000,
        type=int,
        required=False,
    )

    parser.add_argument(
        "-s",
        "--schema-name",
        metavar="SCHEMA-NAME",
        help="Name to use for output files. This is optional if running one instance, but necessary when doing multiple Snakemake runs at once.",
        dest="schema_name",
        type=str,
        required=True,
    )

    parser.add_argument(
        "-t",
        "--nthreads",
        metavar="NUM-PROCESSES",
        help="Number of threads available to multiprocessing module, more threads reduces runtime drastically. Defaults to all available - 1.",
        dest="nthreads",
        type=int,
        required=False,
        default=mp.cpu_count() - 1 or 1,
    )

    parser.add_argument(
        "-o",
        "--out-dir",
        metavar="OUT-DIR",
        help="Directory to write *.npz files to. Defaults to input dir.",
        dest="out_dir",
        type=str,
        required=False,
    )

    args = parser.parse_args()

    return args


def worker(args):
    mutfile, tol, physLen, maxSnps = args

    msh = MsHandler(mutfile, tol, physLen)
    hap_ms, samp_sizes, gens_sampled = msh.parse_slim()
    # print(samp_sizes, gens_sampled)
    # Convert MS into haplotype freq spectrum and format output
    hh = HapHandler(hap_ms, maxSnps, samp_sizes, gens_sampled)
    X, id = hh.readAndSplitMsData(mutfile)
    #! (TPs * sampsize)
    X = np.squeeze(X)

    if X is not None and id is not None:
        return (id, X)


def main():
    argp = parse_arguments()

    print(f"Using {argp.nthreads} threads.")
    print("Data dir:", argp.in_dir)

    # Sanitize output dir
    if argp.out_dir is None:
        out_dir = argp.in_dir
    else:
        out_dir = argp.out_dir
    print("Output dir:", out_dir)

    filelist = glob(os.path.join(argp.in_dir, "*.pop"))
    physLen = argp.physLen
    tol = 0.5  # Allows for infinite sites model
    maxSnps = 51

    id_arrs = []

    args = zip(filelist, cycle([tol]), cycle([physLen]), cycle([maxSnps]))

    chunksize = 4
    pool = mp.Pool(processes=argp.nthreads)
    for proc_result in tqdm(
        pool.imap_unordered(worker, args, chunksize=chunksize),
        desc="Submitting processes...",
        total=len(filelist),
    ):
        id_arrs.append(proc_result)
    # for i in tqdm(args, total=len(filelist)):
    #    id_arrs.append(worker(i))

    ids = []
    arrs = []
    # Have to do sanity check
    for i in id_arrs:
        if i:
            ids.append(i[0])
            arrs.append(i[1])

    print("Number of samples processed:", len(ids))
    print("Shape of single sample:", arrs[0].shape)

    np.savez(
        os.path.join(out_dir, f"hfs_{argp.schema_name}.npz"), **dict(zip(ids, arrs)),
    )
    print(
        "HFS data saved to:", os.path.join(out_dir, f"hfs_{argp.schema_name}.npz"),
    )


if __name__ == "__main__":
    main()
